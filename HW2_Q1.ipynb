{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#open url information\n",
    "fd = open('./data/project_successful_meta.json', 'r')\n",
    "text = fd.read()\n",
    "fd.close()\n",
    "\n",
    "pmeta = json.loads(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Doc():\n",
    "    \n",
    "    def __init__(self, docid, doc, author, year):\n",
    "        self.docid = docid\n",
    "        self.text = doc.lower()\n",
    "        self.text = re.sub(u'[\\u2019\\']', '', self.text)\n",
    "        self.tokens = np.array(wordpunct_tokenize(self.text))\n",
    "        self.stem = None\n",
    "        self.author = author\n",
    "        self.year = year\n",
    "        \n",
    "    def tf(self, wordlist):\n",
    "        \n",
    "        \"\"\"\n",
    "        Returns ARRAY woth wordlist frequency\n",
    "        \"\"\"\n",
    "        \n",
    "        count = np.zeros(len(wordlist))\n",
    "        \n",
    "        for wid, word in np.ndenumerate(wordlist):\n",
    "            count[wid] = (self.tokens == word).sum()\n",
    "        return count\n",
    "        \n",
    "    \n",
    "    def word_exists(self, wordlist):\n",
    "        \n",
    "        \"\"\"\n",
    "        Return dict with log count of wordlist\n",
    "        \"\"\"\n",
    "        \n",
    "        is_word = np.zeros(len(wordlist))\n",
    "        \n",
    "        for wid, word in np.ndenumerate(wordlist):\n",
    "            if word in self.tokens:\n",
    "                is_word[wid] = 1\n",
    "        return is_word\n",
    "            \n",
    "    def token_clean(self,length):\n",
    "\n",
    "        \"\"\" \n",
    "        strip out non-alpha tokens and length one tokens\n",
    "        \"\"\"\n",
    "\n",
    "        self.tokens = np.array([t for t in self.tokens if (t.isalpha() and len(t) > length)])\n",
    "\n",
    "\n",
    "    def stopword_remove(self, stopwords):\n",
    "\n",
    "        \"\"\"\n",
    "        Remove stopwords from tokens.\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        self.tokens = np.array([t for t in self.tokens if t not in stopwords])\n",
    "\n",
    "\n",
    "    def stem(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Stem tokens with Porter Stemmer.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.stems = n.array([PorterStemmer().stem(t) for t in self.tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs,re\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "import numpy as np\n",
    "class RawDocs():\n",
    "    \n",
    "    def __init__(self, doc_data, stopword_file):\n",
    "\n",
    "        self.docs = [Doc(docid, doc[2], doc[1], doc[0]) for docid, doc in enumerate(doc_data)]\n",
    "\n",
    "        with codecs.open(stopword_file,'r','utf-8') as f: raw = f.read()\n",
    "        self.stopwords = set(raw.splitlines())\n",
    "\n",
    "        self.N = len(self.docs)\n",
    "        \n",
    "    def clean_docs(self, length):\n",
    "        \n",
    "        for doc in self.docs:\n",
    "            doc.token_clean(length)\n",
    "            doc.stopword_remove(self.stopwords)\n",
    "            \n",
    "    def count(self, dictionary):\n",
    "        \n",
    "        return ({doc.docid : doc.tf(dictionary) for doc in self.docs})\n",
    "    \n",
    "    def idf(self, dictionary):\n",
    "        \n",
    "        is_word_docs = {doc.docid : doc.word_exists(dictionary) for doc in self.docs}\n",
    "        \n",
    "        return(np.log(self.N / sum([is_word for is_word in is_word_docs.values()])))\n",
    "    \n",
    "    def tf_idf(self, dictionary):\n",
    "        \n",
    "        tf = self.count(dictionary)\n",
    "        idf = self.idf(dictionary)\n",
    "        \n",
    "        td_idf_docs = dict()\n",
    "        #convert counts t log counts\n",
    "        for did in range(self.N):\n",
    "            td_idf_docs[did] = np.log(tf[did] + 1) * idf\n",
    "            \n",
    "        return(td_idf_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "fd = open('./data/pres_speech.json', 'r')\n",
    "text = fd.read()\n",
    "fd.close()\n",
    "\n",
    "pres_speech = json.loads(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "speech_docs = RawDocs(pres_speech[0: 10], './data/stopwords.txt')\n",
    "speech_docs.clean_docs(2)\n",
    "doc1 = speech_docs.docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([  3.,   2.,  24.,   3.,   2.]),\n",
       " 1: array([  3.,   3.,  33.,   5.,   1.]),\n",
       " 2: array([  2.,   2.,  18.,   0.,   1.]),\n",
       " 3: array([  2.,   2.,  18.,   0.,   2.]),\n",
       " 4: array([  2.,   2.,  18.,   9.,   0.]),\n",
       " 5: array([  2.,   2.,  30.,   1.,   2.]),\n",
       " 6: array([  1.,   1.,  15.,   0.,   5.]),\n",
       " 7: array([  2.,   1.,  38.,   0.,   7.]),\n",
       " 8: array([  2.,   1.,  24.,   0.,   2.]),\n",
       " 9: array([  1.,   1.,  30.,   0.,   2.])}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_docs.count(['god', 'bless', 'america', 'race', 'human'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.          0.          0.91629073  0.10536052]\n"
     ]
    }
   ],
   "source": [
    "print(speech_docs.idf(['god', 'bless', 'america', 'race', 'human']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([ 0.        ,  0.        ,  0.        ,  1.27024867,  0.11575036]),\n",
       " 1: array([ 0.        ,  0.        ,  0.        ,  1.6417726 ,  0.07303034]),\n",
       " 2: array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.07303034]),\n",
       " 3: array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.11575036]),\n",
       " 4: array([ 0.        ,  0.        ,  0.        ,  2.10983738,  0.        ]),\n",
       " 5: array([ 0.        ,  0.        ,  0.        ,  0.63512434,  0.11575036]),\n",
       " 6: array([ 0.       ,  0.       ,  0.       ,  0.       ,  0.1887807]),\n",
       " 7: array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.21909103]),\n",
       " 8: array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.11575036]),\n",
       " 9: array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.11575036])}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_docs.tf_idf(['god', 'bless', 'america', 'race', 'human'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  2.,  3.])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([1,2,3])\n",
    "b = np.ones(3)\n",
    "c = [a, b]\n",
    "d = sum(c)\n",
    "\n",
    "a * b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
