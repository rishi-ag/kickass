{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW2  Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class of Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "class Doc():\n",
    "    \n",
    "    \"\"\" The Doc class rpresents a class of individula documents\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, docid, doc, author, year):\n",
    "        self.docid = docid\n",
    "        self.text = doc.lower()\n",
    "        self.text = re.sub(u'[\\u2019\\']', '', self.text)\n",
    "        self.tokens = np.array(wordpunct_tokenize(self.text))\n",
    "        self.stem = None\n",
    "        self.author = author\n",
    "        self.year = year\n",
    "        \n",
    "    def tf(self, wordlist):\n",
    "        \n",
    "        \"\"\"\n",
    "        Returns ARRAY with wordlist frequency\n",
    "        \"\"\"\n",
    "        \n",
    "        count = np.zeros(len(wordlist))\n",
    "        \n",
    "        for wid, word in np.ndenumerate(wordlist):\n",
    "            count[wid] = (self.tokens == word).sum()\n",
    "        return count\n",
    "        \n",
    "    \n",
    "    def word_exists(self, wordlist):\n",
    "        \n",
    "        \"\"\"\n",
    "        Returns ARRAY of binary value where 1 inidicates presence of a word\n",
    "        \"\"\"\n",
    "        \n",
    "        is_word = np.zeros(len(wordlist))\n",
    "        \n",
    "        for wid, word in np.ndenumerate(wordlist):\n",
    "            if word in self.tokens:\n",
    "                is_word[wid] = 1\n",
    "        return is_word\n",
    "            \n",
    "    def token_clean(self,length):\n",
    "\n",
    "        \"\"\" \n",
    "        strip out non-alpha tokens and length one tokens\n",
    "        \"\"\"\n",
    "\n",
    "        self.tokens = np.array([t for t in self.tokens if (t.isalpha() and len(t) > length)])\n",
    "\n",
    "\n",
    "    def stopword_remove(self, stopwords):\n",
    "\n",
    "        \"\"\"\n",
    "        Remove stopwords from tokens.\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        self.tokens = np.array([t for t in self.tokens if t not in stopwords])\n",
    "\n",
    "\n",
    "    def stem(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Stem tokens with Porter Stemmer.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.stems = n.array([PorterStemmer().stem(t) for t in self.tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Class of Document Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "\n",
    "class RawDocs():\n",
    "    \"\"\" The RawDocs class rpresents a class of document collections\n",
    "     \n",
    "    \"\"\"\n",
    "    def __init__(self, doc_data, stopword_file):\n",
    "\n",
    "        self.docs = [Doc(docid, doc[2], doc[1], doc[0]) for docid, doc in enumerate(doc_data)]\n",
    "\n",
    "        with codecs.open(stopword_file,'r','utf-8') as f: raw = f.read()\n",
    "        self.stopwords = set(raw.splitlines())\n",
    "\n",
    "        self.N = len(self.docs)\n",
    "        \n",
    "    def clean_docs(self, length):\n",
    "        \"\"\" \n",
    "        Applies stopword removal, token cleaning to docs\n",
    "        \"\"\"\n",
    "        for doc in self.docs:\n",
    "            doc.token_clean(length)\n",
    "            doc.stopword_remove(self.stopwords)\n",
    "            \n",
    "    def count(self, dictionary):\n",
    "        \"\"\" \n",
    "        word count frequency of dictionary in document collection\n",
    "        \"\"\"\n",
    "        \n",
    "        return ({(doc.docid, doc.year, doc.author) : \\\n",
    "                 doc.tf(dictionary) for doc in self.docs})\n",
    "    \n",
    "    def idf(self, dictionary):\n",
    "        \"\"\" \n",
    "        returns array of inverted document frequency for given dictionary \n",
    "        over collection of docs\n",
    "        \"\"\"\n",
    "        \n",
    "        is_word_docs = np.array([doc.word_exists(dictionary) for doc in self.docs])\n",
    "        \n",
    "        return(np.log(self.N / sum([is_word for is_word in is_word_docs])))\n",
    "    \n",
    "    def tf_idf(self, dictionary):\n",
    "        \"\"\" \n",
    "        returns tf-idf score of given dictionary of words for every document \n",
    "        \"\"\"\n",
    "        \n",
    "        tf = self.count(dictionary)\n",
    "        idf = self.idf(dictionary)\n",
    "        \n",
    "        tf_idf_docs = dict()\n",
    "        #convert counts t log counts\n",
    "        for doc in self.docs:\n",
    "            tf_idf_docs[(doc.docid, doc.year, doc.author) ] = \\\n",
    "            np.log(tf[(doc.docid, doc.year, doc.author)] + 1) * idf\n",
    "            \n",
    "        return(tf_idf_docs)\n",
    "    \n",
    "    def rank_tfidf(self, dictionary):\n",
    "        \n",
    "        \"\"\"\n",
    "        Calculates document rank based on tfidf\n",
    "        \"\"\"\n",
    "        \n",
    "        docs_tfidf = self.tf_idf(dictionary)\n",
    "        doc_rank = []\n",
    "        \n",
    "        for key in docs_tfidf.keys():\n",
    "            docs_tfidf[key] = sum(docs_tfidf[key])\n",
    "            doc_rank.append((key, docs_tfidf[key]))\n",
    "            \n",
    "        return(np.sort(np.array(doc_rank), axis=0)[::-1])\n",
    "    \n",
    "    def rank_count(self, dictionary):\n",
    "        \n",
    "        \"\"\"\n",
    "        Calculates document rank based on word frequency\n",
    "        \"\"\"\n",
    "        \n",
    "        docs_count = self.count(dictionary)\n",
    "        doc_rank = []\n",
    "        \n",
    "        for key in docs_count.keys():\n",
    "            docs_count[key] = sum(docs_count[key])\n",
    "            doc_rank.append((key, docs_count[key]))\n",
    "            \n",
    "        return(np.sort(np.array(doc_rank), axis=0)[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####import documents for consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import documents for consumption\n",
    "import json\n",
    "fd = open('./data/pres_speech.json', 'r')\n",
    "text = fd.read()\n",
    "fd.close()\n",
    "\n",
    "pres_speech = json.loads(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Instantiate RawDocs class and preprocess documents preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "speech_docs = RawDocs(pres_speech[0:20], './data/stopwords.txt')\n",
    "speech_docs.clean_docs(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, '2013', 'Obama'): array([ 0.,  2.,  3.,  2.,  1.,  4.,  2.]),\n",
       " (1, '2014', 'Obama'): array([  1.,   3.,   5.,   1.,   3.,  10.,   0.]),\n",
       " (2, '2009', 'Obama'): array([ 0.,  2.,  0.,  1.,  1.,  2.,  0.]),\n",
       " (3, '2010', 'Obama'): array([ 1.,  2.,  0.,  2.,  2.,  3.,  1.]),\n",
       " (4, '2011', 'Obama'): array([ 0.,  2.,  9.,  0.,  0.,  8.,  0.]),\n",
       " (5, '2012', 'Obama'): array([ 1.,  2.,  1.,  2.,  0.,  3.,  2.]),\n",
       " (6, '2005', 'Bush'): array([ 1.,  1.,  0.,  5.,  2.,  2.,  0.]),\n",
       " (7, '2006', 'Bush'): array([ 1.,  1.,  0.,  7.,  4.,  2.,  1.]),\n",
       " (8, '2007', 'Bush'): array([ 3.,  1.,  0.,  2.,  0.,  2.,  0.]),\n",
       " (9, '2008', 'Bush'): array([ 1.,  1.,  0.,  2.,  0.,  2.,  1.]),\n",
       " (10, '2001', 'Bush'): array([ 0.,  1.,  0.,  0.,  0.,  1.,  0.]),\n",
       " (11, '2002', 'Bush'): array([ 4.,  1.,  1.,  2.,  1.,  1.,  0.]),\n",
       " (12, '2003', 'Bush'): array([ 3.,  1.,  0.,  5.,  2.,  1.,  0.]),\n",
       " (13, '2004', 'Bush'): array([ 5.,  1.,  1.,  1.,  1.,  3.,  1.]),\n",
       " (14, '1997', 'Clinton'): array([ 0.,  2.,  0.,  3.,  0.,  7.,  0.]),\n",
       " (15, '1998', 'Clinton'): array([ 0.,  2.,  3.,  3.,  0.,  3.,  1.]),\n",
       " (16, '1999', 'Clinton'): array([ 0.,  0.,  2.,  1.,  0.,  4.,  0.]),\n",
       " (17, '2000', 'Clinton'): array([ 1.,  2.,  4.,  3.,  0.,  7.,  0.]),\n",
       " (18, '1993', 'Clinton'): array([  0.,   1.,   0.,   0.,   1.,  14.,   2.]),\n",
       " (19, '1994', 'Clinton'): array([ 0.,  2.,  0.,  2.,  2.,  4.,  2.])}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_list = ['danger', 'bless', 'race', 'human', 'fear', 'believe', 'influence']\n",
    "\n",
    "speech_docs.count(sample_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0,\n",
       "  '2013',\n",
       "  'Obama'): array([ 0.        ,  0.05635144,  1.10696672,  0.17854529,  0.41438903,\n",
       "         0.        ,  0.87725037]),\n",
       " (1,\n",
       "  '2014',\n",
       "  'Obama'): array([ 0.41438903,  0.0711076 ,  1.43073373,  0.11264954,  0.82877806,\n",
       "         0.        ,  0.        ]),\n",
       " (2,\n",
       "  '2009',\n",
       "  'Obama'): array([ 0.        ,  0.05635144,  0.        ,  0.11264954,  0.41438903,\n",
       "         0.        ,  0.        ]),\n",
       " (3,\n",
       "  '2010',\n",
       "  'Obama'): array([ 0.41438903,  0.05635144,  0.        ,  0.17854529,  0.65679108,\n",
       "         0.        ,  0.55348336]),\n",
       " (4,\n",
       "  '2011',\n",
       "  'Obama'): array([ 0.        ,  0.05635144,  1.83863192,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " (5,\n",
       "  '2012',\n",
       "  'Obama'): array([ 0.41438903,  0.05635144,  0.55348336,  0.17854529,  0.        ,\n",
       "         0.        ,  0.87725037]),\n",
       " (6,\n",
       "  '2005',\n",
       "  'Bush'): array([ 0.41438903,  0.0355538 ,  0.        ,  0.29119483,  0.65679108,\n",
       "         0.        ,  0.        ]),\n",
       " (7,\n",
       "  '2006',\n",
       "  'Bush'): array([ 0.41438903,  0.0355538 ,  0.        ,  0.33794861,  0.96218153,\n",
       "         0.        ,  0.55348336]),\n",
       " (8,\n",
       "  '2007',\n",
       "  'Bush'): array([ 0.82877806,  0.0355538 ,  0.        ,  0.17854529,  0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " (9,\n",
       "  '2008',\n",
       "  'Bush'): array([ 0.41438903,  0.0355538 ,  0.        ,  0.17854529,  0.        ,\n",
       "         0.        ,  0.55348336]),\n",
       " (10,\n",
       "  '2001',\n",
       "  'Bush'): array([ 0.       ,  0.0355538,  0.       ,  0.       ,  0.       ,\n",
       "         0.       ,  0.       ]),\n",
       " (11,\n",
       "  '2002',\n",
       "  'Bush'): array([ 0.96218153,  0.0355538 ,  0.55348336,  0.17854529,  0.41438903,\n",
       "         0.        ,  0.        ]),\n",
       " (12,\n",
       "  '2003',\n",
       "  'Bush'): array([ 0.82877806,  0.0355538 ,  0.        ,  0.29119483,  0.65679108,\n",
       "         0.        ,  0.        ]),\n",
       " (13,\n",
       "  '2004',\n",
       "  'Bush'): array([ 1.07118011,  0.0355538 ,  0.55348336,  0.11264954,  0.41438903,\n",
       "         0.        ,  0.55348336]),\n",
       " (14,\n",
       "  '1997',\n",
       "  'Clinton'): array([ 0.        ,  0.05635144,  0.        ,  0.22529908,  0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " (15,\n",
       "  '1998',\n",
       "  'Clinton'): array([ 0.        ,  0.05635144,  1.10696672,  0.22529908,  0.        ,\n",
       "         0.        ,  0.55348336]),\n",
       " (16,\n",
       "  '1999',\n",
       "  'Clinton'): array([ 0.        ,  0.        ,  0.87725037,  0.11264954,  0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " (17,\n",
       "  '2000',\n",
       "  'Clinton'): array([ 0.41438903,  0.05635144,  1.28514856,  0.22529908,  0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " (18,\n",
       "  '1993',\n",
       "  'Clinton'): array([ 0.        ,  0.0355538 ,  0.        ,  0.        ,  0.41438903,\n",
       "         0.        ,  0.87725037]),\n",
       " (19,\n",
       "  '1994',\n",
       "  'Clinton'): array([ 0.        ,  0.05635144,  0.        ,  0.17854529,  0.65679108,\n",
       "         0.        ,  0.87725037])}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_docs.tf_idf(sample_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[(19, '1994', 'Clinton'), 2.8576579630159151],\n",
       "       [(18, '1993', 'Clinton'), 2.7407391953998514],\n",
       "       [(17, '2000', 'Clinton'), 2.6335028523744848],\n",
       "       [(16, '1999', 'Clinton'), 2.3035563399622987],\n",
       "       [(15, '1998', 'Clinton'), 2.1441530197427303],\n",
       "       [(14, '1997', 'Clinton'), 2.0800194940857195],\n",
       "       [(13, '2004', 'Bush'), 1.9811881102499056],\n",
       "       [(12, '2003', 'Bush'), 1.9421005939447376],\n",
       "       [(11, '2002', 'Bush'), 1.8949833614924929],\n",
       "       [(10, '2001', 'Bush'), 1.8595602020754241],\n",
       "       [(9, '2008', 'Bush'), 1.8123177719097618],\n",
       "       [(8, '2007', 'Bush'), 1.7689381799393922],\n",
       "       [(7, '2006', 'Bush'), 1.3979287404015897],\n",
       "       [(6, '2005', 'Bush'), 1.3271932015554397],\n",
       "       [(5, '2012', 'Obama'), 1.1819714852707415],\n",
       "       [(4, '2011', 'Obama'), 1.0428771584901479],\n",
       "       [(3, '2010', 'Obama'), 0.98989990542990902],\n",
       "       [(2, '2009', 'Obama'), 0.58339001281760938],\n",
       "       [(1, '2014', 'Obama'), 0.28165051907844041],\n",
       "       [(0, '2013', 'Obama'), 0.035553802386361882]], dtype=object)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_docs.rank_tfidf(sample_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[(19, '1994', 'Clinton'), 23.0],\n",
       "       [(18, '1993', 'Clinton'), 19.0],\n",
       "       [(17, '2000', 'Clinton'), 18.0],\n",
       "       [(16, '1999', 'Clinton'), 17.0],\n",
       "       [(15, '1998', 'Clinton'), 16.0],\n",
       "       [(14, '1997', 'Clinton'), 14.0],\n",
       "       [(13, '2004', 'Bush'), 13.0],\n",
       "       [(12, '2003', 'Bush'), 12.0],\n",
       "       [(11, '2002', 'Bush'), 12.0],\n",
       "       [(10, '2001', 'Bush'), 12.0],\n",
       "       [(9, '2008', 'Bush'), 12.0],\n",
       "       [(8, '2007', 'Bush'), 11.0],\n",
       "       [(7, '2006', 'Bush'), 11.0],\n",
       "       [(6, '2005', 'Bush'), 11.0],\n",
       "       [(5, '2012', 'Obama'), 10.0],\n",
       "       [(4, '2011', 'Obama'), 8.0],\n",
       "       [(3, '2010', 'Obama'), 7.0],\n",
       "       [(2, '2009', 'Obama'), 7.0],\n",
       "       [(1, '2014', 'Obama'), 6.0],\n",
       "       [(0, '2013', 'Obama'), 2.0]], dtype=object)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_docs.rank_count(sample_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
