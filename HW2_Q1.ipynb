{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW2  Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class of Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "class Doc():\n",
    "    \n",
    "    \"\"\" The Doc class rpresents a class of individula documents\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, docid, doc, author, year):\n",
    "        self.docid = docid\n",
    "        self.text = doc.lower()\n",
    "        self.text = re.sub(u'[\\u2019\\']', '', self.text)\n",
    "        self.tokens = np.array(wordpunct_tokenize(self.text))\n",
    "        self.stem = None\n",
    "        self.author = author\n",
    "        self.year = year\n",
    "        \n",
    "    def tf(self, wordlist):\n",
    "        \n",
    "        \"\"\"\n",
    "        Returns ARRAY with wordlist frequency\n",
    "        \"\"\"\n",
    "        \n",
    "        count = np.zeros(len(wordlist))\n",
    "        \n",
    "        for wid, word in np.ndenumerate(wordlist):\n",
    "            count[wid] = (self.tokens == word).sum()\n",
    "        return count\n",
    "        \n",
    "    \n",
    "    def word_exists(self, wordlist):\n",
    "        \n",
    "        \"\"\"\n",
    "        Returns ARRAY of binary value where 1 inidicates presence of a word\n",
    "        \"\"\"\n",
    "        \n",
    "        is_word = np.zeros(len(wordlist))\n",
    "        \n",
    "        for wid, word in np.ndenumerate(wordlist):\n",
    "            if word in self.tokens:\n",
    "                is_word[wid] = 1\n",
    "        return is_word\n",
    "            \n",
    "    def token_clean(self,length):\n",
    "\n",
    "        \"\"\" \n",
    "        strip out non-alpha tokens and length one tokens\n",
    "        \"\"\"\n",
    "\n",
    "        self.tokens = np.array([t for t in self.tokens if (t.isalpha() and len(t) > length)])\n",
    "\n",
    "\n",
    "    def stopword_remove(self, stopwords):\n",
    "\n",
    "        \"\"\"\n",
    "        Remove stopwords from tokens.\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        self.tokens = np.array([t for t in self.tokens if t not in stopwords])\n",
    "\n",
    "\n",
    "    def stem(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Stem tokens with Porter Stemmer.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.stems = n.array([PorterStemmer().stem(t) for t in self.tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Class of Document Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "\n",
    "class RawDocs():\n",
    "    \"\"\" The RawDocs class rpresents a class of document collections\n",
    "     \n",
    "    \"\"\"\n",
    "    def __init__(self, doc_data, stopword_file):\n",
    "\n",
    "        self.docs = [Doc(docid, doc[2], doc[1], doc[0]) for docid, doc in enumerate(doc_data)]\n",
    "\n",
    "        with codecs.open(stopword_file,'r','utf-8') as f: raw = f.read()\n",
    "        self.stopwords = set(raw.splitlines())\n",
    "\n",
    "        self.N = len(self.docs)\n",
    "        \n",
    "    def clean_docs(self, length):\n",
    "        \"\"\" \n",
    "        Applies stopword removal, token cleaning to docs\n",
    "        \"\"\"\n",
    "        for doc in self.docs:\n",
    "            doc.token_clean(length)\n",
    "            doc.stopword_remove(self.stopwords)\n",
    "            \n",
    "    def count(self, dictionary):\n",
    "        \"\"\" \n",
    "        word count frequency of dictionary in document collection\n",
    "        \"\"\"\n",
    "        \n",
    "        return ({(doc.docid, doc.year, doc.author) : \\\n",
    "                 doc.tf(dictionary) for doc in self.docs})\n",
    "    \n",
    "    def idf(self, dictionary):\n",
    "        \"\"\" \n",
    "        returns array of inverted document frequency for given dictionary \n",
    "        over collection of docs\n",
    "        \"\"\"\n",
    "        \n",
    "        is_word_docs = np.array([doc.word_exists(dictionary) for doc in self.docs])\n",
    "        \n",
    "        return(np.log(self.N / sum([is_word for is_word in is_word_docs])))\n",
    "    \n",
    "    def tf_idf(self, dictionary):\n",
    "        \"\"\" \n",
    "        returns tf-idf score of given dictionary of words for every document \n",
    "        \"\"\"\n",
    "        \n",
    "        tf = self.count(dictionary)\n",
    "        idf = self.idf(dictionary)\n",
    "        \n",
    "        tf_idf_docs = dict()\n",
    "        #convert counts t log counts\n",
    "        for doc in self.docs:\n",
    "            tf_idf_docs[(doc.docid, doc.year, doc.author) ] = \\\n",
    "            np.log(tf[(doc.docid, doc.year, doc.author)] + 1) * idf\n",
    "            \n",
    "        return(tf_idf_docs)\n",
    "    \n",
    "    def rank_tfidf(self, dictionary):\n",
    "        \n",
    "        \"\"\"\n",
    "        Calculates document rank based on tfidf\n",
    "        \"\"\"\n",
    "        \n",
    "        docs_tfidf = self.tf_idf(dictionary)\n",
    "        \n",
    "        doc_rank = [[key, sum(docs_tfidf[key])] for key in docs_tfidf.keys()]\n",
    "            \n",
    "            \n",
    "        return(np.sort(np.array(doc_rank), axis=0)[::-1])\n",
    "    \n",
    "    def rank_count(self, dictionary):\n",
    "        \n",
    "        \"\"\"\n",
    "        Calculates document rank based on word frequency\n",
    "        \"\"\"\n",
    "        \n",
    "        docs_count = self.count(dictionary)\n",
    "        \n",
    "        doc_rank = [[key, sum(docs_count[key])] for key in docs_count.keys()]\n",
    "                    \n",
    "        return(np.sort(np.array(doc_rank), axis=0)[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####import documents for consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import documents for consumption\n",
    "import json\n",
    "fd = open('./data/pres_speech.json', 'r')\n",
    "text = fd.read()\n",
    "fd.close()\n",
    "\n",
    "pres_speech = json.loads(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Instantiate RawDocs class and preprocess documents preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "speech_docs = RawDocs(pres_speech[0:20], './data/stopwords.txt')\n",
    "speech_docs.clean_docs(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, '2013', 'Obama'): array([ 0.,  3.,  2.,  1.,  2.,  1.,  1.,  3.,  0.]),\n",
       " (1, '2014', 'Obama'): array([ 1.,  5.,  1.,  3.,  0.,  3.,  1.,  7.,  1.]),\n",
       " (2, '2009', 'Obama'): array([ 0.,  0.,  1.,  1.,  0.,  1.,  0.,  5.,  0.]),\n",
       " (3, '2010', 'Obama'): array([ 1.,  0.,  2.,  2.,  1.,  2.,  2.,  4.,  0.]),\n",
       " (4, '2011', 'Obama'): array([ 0.,  9.,  0.,  0.,  0.,  1.,  3.,  4.,  0.]),\n",
       " (5, '2012', 'Obama'): array([ 1.,  1.,  2.,  0.,  2.,  0.,  0.,  6.,  1.]),\n",
       " (6,\n",
       "  '2005',\n",
       "  'Bush'): array([  1.,   0.,   5.,   2.,   0.,  11.,   0.,   5.,   2.]),\n",
       " (7, '2006', 'Bush'): array([ 1.,  0.,  7.,  4.,  1.,  6.,  3.,  2.,  1.]),\n",
       " (8,\n",
       "  '2007',\n",
       "  'Bush'): array([  3.,   0.,   2.,   0.,   0.,   3.,   2.,  10.,   0.]),\n",
       " (9, '2008', 'Bush'): array([ 1.,  0.,  2.,  0.,  1.,  8.,  0.,  3.,  2.]),\n",
       " (10, '2001', 'Bush'): array([ 0.,  0.,  0.,  0.,  0.,  6.,  1.,  1.,  0.]),\n",
       " (11,\n",
       "  '2002',\n",
       "  'Bush'): array([  4.,   1.,   2.,   1.,   0.,   4.,   2.,  12.,   2.]),\n",
       " (12,\n",
       "  '2003',\n",
       "  'Bush'): array([  3.,   0.,   5.,   2.,   0.,   9.,   0.,  11.,   0.]),\n",
       " (13,\n",
       "  '2004',\n",
       "  'Bush'): array([  5.,   1.,   1.,   1.,   1.,   4.,   2.,  12.,   2.]),\n",
       " (14, '1997', 'Clinton'): array([ 0.,  0.,  3.,  0.,  0.,  9.,  0.,  7.,  0.]),\n",
       " (15, '1998', 'Clinton'): array([ 0.,  3.,  3.,  0.,  1.,  7.,  2.,  3.,  0.]),\n",
       " (16, '1999', 'Clinton'): array([ 0.,  2.,  1.,  0.,  0.,  8.,  1.,  5.,  0.]),\n",
       " (17, '2000', 'Clinton'): array([ 1.,  4.,  3.,  0.,  0.,  8.,  0.,  3.,  0.]),\n",
       " (18, '1993', 'Clinton'): array([ 0.,  0.,  0.,  1.,  2.,  3.,  0.,  1.,  0.]),\n",
       " (19, '1994', 'Clinton'): array([ 0.,  0.,  2.,  2.,  2.,  5.,  0.,  2.,  0.])}"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_list = ['danger', 'race', 'human', 'fear', 'influence', 'peace', 'love', 'war', 'tyranny']\n",
    "\n",
    "speech_docs.count(sample_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0,\n",
       "  '2013',\n",
       "  'Obama'): array([ 0.        ,  1.10696672,  0.17854529,  0.41438903,  0.87725037,\n",
       "         0.0355538 ,  0.41438903,  0.        ,  0.        ]),\n",
       " (1,\n",
       "  '2014',\n",
       "  'Obama'): array([ 0.41438903,  1.43073373,  0.11264954,  0.82877806,  0.        ,\n",
       "         0.0711076 ,  0.41438903,  0.        ,  0.72768125]),\n",
       " (2,\n",
       "  '2009',\n",
       "  'Obama'): array([ 0.        ,  0.        ,  0.11264954,  0.41438903,  0.        ,\n",
       "         0.0355538 ,  0.        ,  0.        ,  0.        ]),\n",
       " (3,\n",
       "  '2010',\n",
       "  'Obama'): array([ 0.41438903,  0.        ,  0.17854529,  0.65679108,  0.55348336,\n",
       "         0.05635144,  0.65679108,  0.        ,  0.        ]),\n",
       " (4,\n",
       "  '2011',\n",
       "  'Obama'): array([ 0.        ,  1.83863192,  0.        ,  0.        ,  0.        ,\n",
       "         0.0355538 ,  0.82877806,  0.        ,  0.        ]),\n",
       " (5,\n",
       "  '2012',\n",
       "  'Obama'): array([ 0.41438903,  0.55348336,  0.17854529,  0.        ,  0.87725037,\n",
       "         0.        ,  0.        ,  0.        ,  0.72768125]),\n",
       " (6,\n",
       "  '2005',\n",
       "  'Bush'): array([ 0.41438903,  0.        ,  0.29119483,  0.65679108,  0.        ,\n",
       "         0.12745905,  0.        ,  0.        ,  1.15334749]),\n",
       " (7,\n",
       "  '2006',\n",
       "  'Bush'): array([ 0.41438903,  0.        ,  0.33794861,  0.96218153,  0.55348336,\n",
       "         0.09981214,  0.82877806,  0.        ,  0.72768125]),\n",
       " (8,\n",
       "  '2007',\n",
       "  'Bush'): array([ 0.82877806,  0.        ,  0.17854529,  0.        ,  0.        ,\n",
       "         0.0711076 ,  0.65679108,  0.        ,  0.        ]),\n",
       " (9,\n",
       "  '2008',\n",
       "  'Bush'): array([ 0.41438903,  0.        ,  0.17854529,  0.        ,  0.55348336,\n",
       "         0.11270289,  0.        ,  0.        ,  1.15334749]),\n",
       " (10,\n",
       "  '2001',\n",
       "  'Bush'): array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.09981214,  0.41438903,  0.        ,  0.        ]),\n",
       " (11,\n",
       "  '2002',\n",
       "  'Bush'): array([ 0.96218153,  0.55348336,  0.17854529,  0.41438903,  0.        ,\n",
       "         0.08255337,  0.65679108,  0.        ,  1.15334749]),\n",
       " (12,\n",
       "  '2003',\n",
       "  'Bush'): array([ 0.82877806,  0.        ,  0.29119483,  0.65679108,  0.        ,\n",
       "         0.11810718,  0.        ,  0.        ,  0.        ]),\n",
       " (13,\n",
       "  '2004',\n",
       "  'Bush'): array([ 1.07118011,  0.55348336,  0.11264954,  0.41438903,  0.55348336,\n",
       "         0.08255337,  0.65679108,  0.        ,  1.15334749]),\n",
       " (14,\n",
       "  '1997',\n",
       "  'Clinton'): array([ 0.        ,  0.        ,  0.22529908,  0.        ,  0.        ,\n",
       "         0.11810718,  0.        ,  0.        ,  0.        ]),\n",
       " (15,\n",
       "  '1998',\n",
       "  'Clinton'): array([ 0.        ,  1.10696672,  0.22529908,  0.        ,  0.55348336,\n",
       "         0.10666141,  0.65679108,  0.        ,  0.        ]),\n",
       " (16,\n",
       "  '1999',\n",
       "  'Clinton'): array([ 0.        ,  0.87725037,  0.11264954,  0.        ,  0.        ,\n",
       "         0.11270289,  0.41438903,  0.        ,  0.        ]),\n",
       " (17,\n",
       "  '2000',\n",
       "  'Clinton'): array([ 0.41438903,  1.28514856,  0.22529908,  0.        ,  0.        ,\n",
       "         0.11270289,  0.        ,  0.        ,  0.        ]),\n",
       " (18,\n",
       "  '1993',\n",
       "  'Clinton'): array([ 0.        ,  0.        ,  0.        ,  0.41438903,  0.87725037,\n",
       "         0.0711076 ,  0.        ,  0.        ,  0.        ]),\n",
       " (19,\n",
       "  '1994',\n",
       "  'Clinton'): array([ 0.        ,  0.        ,  0.17854529,  0.65679108,  0.87725037,\n",
       "         0.09190525,  0.        ,  0.        ,  0.        ])}"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_docs.tf_idf(sample_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[(19, '1994', 'Clinton'), 4.5978773281949756],\n",
       "       [(18, '1993', 'Clinton'), 4.0012911525378563],\n",
       "       [(17, '2000', 'Clinton'), 3.999728240209798],\n",
       "       [(16, '1999', 'Clinton'), 3.9242739884052078],\n",
       "       [(15, '1998', 'Clinton'), 3.0270942427285847],\n",
       "       [(14, '1997', 'Clinton'), 2.7513492962309956],\n",
       "       [(13, '2004', 'Bush'), 2.7029637833547655],\n",
       "       [(12, '2003', 'Bush'), 2.649201633214],\n",
       "       [(11, '2002', 'Bush'), 2.643181473218295],\n",
       "       [(10, '2001', 'Bush'), 2.5163512777260344],\n",
       "       [(9, '2008', 'Bush'), 2.4124680568551571],\n",
       "       [(8, '2007', 'Bush'), 2.0375395537903396],\n",
       "       [(7, '2006', 'Bush'), 1.8948711445507287],\n",
       "       [(6, '2005', 'Bush'), 1.8044919823257539],\n",
       "       [(5, '2012', 'Obama'), 1.7352220365271203],\n",
       "       [(4, '2011', 'Obama'), 1.5169918240189491],\n",
       "       [(3, '2010', 'Obama'), 1.3627470039418017],\n",
       "       [(2, '2009', 'Obama'), 0.56259237166353726],\n",
       "       [(1, '2014', 'Obama'), 0.51420117363538864],\n",
       "       [(0, '2013', 'Obama'), 0.34340625056533536]], dtype=object)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_docs.rank_tfidf(sample_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[(19, '1994', 'Clinton'), 30.0],\n",
       "       [(18, '1993', 'Clinton'), 29.0],\n",
       "       [(17, '2000', 'Clinton'), 28.0],\n",
       "       [(16, '1999', 'Clinton'), 26.0],\n",
       "       [(15, '1998', 'Clinton'), 25.0],\n",
       "       [(14, '1997', 'Clinton'), 22.0],\n",
       "       [(13, '2004', 'Bush'), 20.0],\n",
       "       [(12, '2003', 'Bush'), 19.0],\n",
       "       [(11, '2002', 'Bush'), 19.0],\n",
       "       [(10, '2001', 'Bush'), 19.0],\n",
       "       [(9, '2008', 'Bush'), 17.0],\n",
       "       [(8, '2007', 'Bush'), 17.0],\n",
       "       [(7, '2006', 'Bush'), 17.0],\n",
       "       [(6, '2005', 'Bush'), 14.0],\n",
       "       [(5, '2012', 'Obama'), 13.0],\n",
       "       [(4, '2011', 'Obama'), 13.0],\n",
       "       [(3, '2010', 'Obama'), 13.0],\n",
       "       [(2, '2009', 'Obama'), 8.0],\n",
       "       [(1, '2014', 'Obama'), 8.0],\n",
       "       [(0, '2013', 'Obama'), 7.0]], dtype=object)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_docs.rank_count(sample_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8044919899999998"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.array([ 0,  0,  0.17854529,  0.65679108,  0.87725037,  0.09190525,  0,  0,  0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
