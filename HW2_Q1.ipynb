{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#open url information\n",
      "fd = open('./data/project_successful_meta.json', 'r')\n",
      "text = fd.read()\n",
      "fd.close()\n",
      "\n",
      "pmeta = json.loads(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from math import log\n",
      "\n",
      "class Doc():\n",
      "    \n",
      "    def __init__(self, doc):\n",
      "        self.text = doc[2].lower()\n",
      "        self.text = re.sub(u'[\\u2019\\']', '', self.text)\n",
      "        self.tokens = wordpunct_tokenize(self.text)\n",
      "        self.stem = None\n",
      "        \n",
      "    def count(self, wordlist):\n",
      "        \n",
      "        \"\"\"\n",
      "        Returns dict woth wordlist frequency\n",
      "        \"\"\"\n",
      "        \n",
      "        count = dict()\n",
      "        for word in wordlist:\n",
      "            if word in count.keys():\n",
      "                count[word] += 1\n",
      "            else:\n",
      "                count[word] =  0\n",
      "        return count\n",
      "        \n",
      "    \n",
      "    def log_count(self, wordlist):\n",
      "        \n",
      "        \"\"\"\n",
      "        Return dict with log count of wordlist\n",
      "        \"\"\"\n",
      "        \n",
      "        self.log_count = get_count()\n",
      "        for token in cnt.keys():\n",
      "            self.log_count[token] = log(1 + self.log_count[token])\n",
      "            \n",
      "    def token_clean(self,length):\n",
      "\n",
      "\t\t\"\"\" \n",
      "\t\tstrip out non-alpha tokens and length one tokens\n",
      "\t\t\"\"\"\n",
      "\n",
      "\t\tself.tokens = [t for t in self.tokens if (t.isalpha() and len(t) > length)]\n",
      "\n",
      "\n",
      "    def stopword_remove(self, stopwords):\n",
      "\n",
      "\t\t\"\"\"\n",
      "\t\tRemove stopwords from tokens.\n",
      "\t\t\"\"\"\n",
      "\n",
      "\t\t\n",
      "\t\tself.tokens = [t for t in self.tokens if t not in stopwords]\n",
      "\n",
      "\n",
      "    def stem(self):\n",
      "\n",
      "\t\t\"\"\"\n",
      "\t\tStem tokens with Porter Stemmer.\n",
      "\t\t\"\"\"\n",
      "\n",
      "\t\t\n",
      "\t\tself.stems = [PorterStemmer().stem(t) for t in self.tokens]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 122
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import codecs,re\n",
      "from nltk.tokenize import wordpunct_tokenize\n",
      "from nltk import PorterStemmer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class DocsCollection():\n",
      "    \n",
      "    def __init__(self, doc_data, stopword_file):\n",
      "\n",
      "\t\tself.docs = [Doc(doc) for doc in doc_data]\n",
      "\n",
      "\t\twith codecs.open(stopword_file,'r','utf-8') as f: raw = f.read()\n",
      "\t\tself.stopwords = set(raw.splitlines())\n",
      "\n",
      "\t\tself.N = len(self.docs)\n",
      "        \n",
      "    def clean_docs(self, length):\n",
      "        for doc in self.docs:\n",
      "            doc.token_clean(length)\n",
      "            doc.stopword_remove(self.stopwords)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 87
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "fd = open('./data/pres_speech.json', 'r')\n",
      "text = fd.read()\n",
      "fd.close()\n",
      "\n",
      "pres_speech = json.loads(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "speech_docs = DocsCollection(pres_speech[0: 2], './data/stopwords.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 123
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "speech_docs.clean_docs(2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 124
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "speech_docs.docs[0].count(['ansbdhsbd'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 125,
       "text": [
        "{'ansbdhsbd': 0}"
       ]
      }
     ],
     "prompt_number": 125
    }
   ],
   "metadata": {}
  }
 ]
}