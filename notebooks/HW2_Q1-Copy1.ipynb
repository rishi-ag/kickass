{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#open url information\n",
    "fd = open('./data/project_successful_meta.json', 'r')\n",
    "text = fd.read()\n",
    "fd.close()\n",
    "\n",
    "pmeta = json.loads(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "class Doc():\n",
    "    \n",
    "    def __init__(self, docid, doc, author, year):\n",
    "        self.docid = docid\n",
    "        self.text = doc.lower()\n",
    "        self.text = re.sub(u'[\\u2019\\']', '', self.text)\n",
    "        self.tokens = wordpunct_tokenize(self.text)\n",
    "        self.stem = None\n",
    "        self.author = author\n",
    "        self.year = year\n",
    "        \n",
    "    def count(self, wordlist):\n",
    "        \n",
    "        \"\"\"\n",
    "        Returns dict woth wordlist frequency\n",
    "        \"\"\"\n",
    "        \n",
    "        count = {word:0 for word in wordlist}\n",
    "        \n",
    "        for token in self.tokens:\n",
    "            if token in wordlist:\n",
    "                count[token] += 1\n",
    "                \n",
    "        return count\n",
    "        \n",
    "    \n",
    "    def log_count(self, wordlist):\n",
    "        \n",
    "        \"\"\"\n",
    "        Return dict with log count of wordlist\n",
    "        \"\"\"\n",
    "        \n",
    "        log_count = self.count(wordlist)\n",
    "        for token in log_count.keys():\n",
    "            log_count[token] = log(1 + log_count[token])\n",
    "        return(log_count)\n",
    "            \n",
    "    def token_clean(self,length):\n",
    "\n",
    "        \"\"\" \n",
    "        strip out non-alpha tokens and length one tokens\n",
    "        \"\"\"\n",
    "\n",
    "        self.tokens = [t for t in self.tokens if (t.isalpha() and len(t) > length)]\n",
    "\n",
    "\n",
    "    def stopword_remove(self, stopwords):\n",
    "\n",
    "        \"\"\"\n",
    "        Remove stopwords from tokens.\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        self.tokens = [t for t in self.tokens if t not in stopwords]\n",
    "\n",
    "\n",
    "    def stem(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Stem tokens with Porter Stemmer.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.stems = [PorterStemmer().stem(t) for t in self.tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs,re\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DocsCollection():\n",
    "    \n",
    "    def __init__(self, doc_data, stopword_file):\n",
    "\n",
    "        self.docs = [Doc(docid, doc[2], doc[1], doc[0]) for docid, doc in enumerate(doc_data)]\n",
    "\n",
    "        with codecs.open(stopword_file,'r','utf-8') as f: raw = f.read()\n",
    "        self.stopwords = set(raw.splitlines())\n",
    "\n",
    "        self.N = len(self.docs)\n",
    "        \n",
    "    def clean_docs(self, length):\n",
    "        for doc in self.docs:\n",
    "            doc.token_clean(length)\n",
    "            doc.stopword_remove(self.stopwords)\n",
    "            \n",
    "    def count(self, dictionary):\n",
    "        return ({doc.docid : doc.count(dictionary) for doc in self.docs})\n",
    "    \n",
    "    def log_count(self, dictionary):\n",
    "        return ({doc.docid : doc.log_count(dictionary) for doc in self.docs})\n",
    "    \n",
    "    def idf(self, dictionary):\n",
    "        return({ for doc in self.docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "fd = open('./data/pres_speech.json', 'r')\n",
    "text = fd.read()\n",
    "fd.close()\n",
    "\n",
    "pres_speech = json.loads(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "speech_docs = DocsCollection(pres_speech[0: 10], './data/stopwords.txt')\n",
    "speech_docs.clean_docs(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'please',\n",
       " u'everybody',\n",
       " u'seat',\n",
       " u'speaker',\n",
       " u'vice',\n",
       " u'president',\n",
       " u'members',\n",
       " u'congress',\n",
       " u'fellow',\n",
       " u'americans']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_docs.docs[0].tokens[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'america': 3.2188758248682006, 'courage': 1.3862943611198906, 'sex': 0.0},\n",
       " 1: {'america': 3.5263605246161616, 'courage': 0.6931471805599453, 'sex': 0.0},\n",
       " 2: {'america': 2.9444389791664403, 'courage': 0.0, 'sex': 0.0},\n",
       " 3: {'america': 2.9444389791664403, 'courage': 0.6931471805599453, 'sex': 0.0},\n",
       " 4: {'america': 2.9444389791664403, 'courage': 0.0, 'sex': 0.0},\n",
       " 5: {'america': 3.4339872044851463, 'courage': 0.6931471805599453, 'sex': 0.0},\n",
       " 6: {'america': 2.772588722239781, 'courage': 1.0986122886681098, 'sex': 0.0},\n",
       " 7: {'america': 3.6635616461296463, 'courage': 1.6094379124341003, 'sex': 0.0},\n",
       " 8: {'america': 3.2188758248682006, 'courage': 1.6094379124341003, 'sex': 0.0},\n",
       " 9: {'america': 3.4339872044851463, 'courage': 1.3862943611198906, 'sex': 0.0}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_docs.log_count(['america', 'sex', 'courage'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
