{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#open url information\n",
    "fd = open('./data/project_successful_meta.json', 'r')\n",
    "text = fd.read()\n",
    "fd.close()\n",
    "\n",
    "pmeta = json.loads(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "class Doc():\n",
    "    \n",
    "    def __init__(self, doc, author, year):\n",
    "        self.text = doc.lower()\n",
    "        self.text = re.sub(u'[\\u2019\\']', '', self.text)\n",
    "        self.tokens = wordpunct_tokenize(self.text)\n",
    "        self.stem = None\n",
    "        self.author = author\n",
    "        self.year = year\n",
    "        \n",
    "    def count(self, wordlist):\n",
    "        \n",
    "        \"\"\"\n",
    "        Returns dict woth wordlist frequency\n",
    "        \"\"\"\n",
    "        \n",
    "        count = {word:0 for word in wordlist}\n",
    "        \n",
    "        for token in self.tokens:\n",
    "            if token in wordlist:\n",
    "                count[token] += 1\n",
    "                \n",
    "        return count\n",
    "        \n",
    "    \n",
    "    def log_count(self, wordlist):\n",
    "        \n",
    "        \"\"\"\n",
    "        Return dict with log count of wordlist\n",
    "        \"\"\"\n",
    "        \n",
    "        self.log_count = get_count()\n",
    "        for token in cnt.keys():\n",
    "            self.log_count[token] = log(1 + self.log_count[token])\n",
    "            \n",
    "    def token_clean(self,length):\n",
    "\n",
    "        \"\"\" \n",
    "        strip out non-alpha tokens and length one tokens\n",
    "        \"\"\"\n",
    "\n",
    "        self.tokens = [t for t in self.tokens if (t.isalpha() and len(t) > length)]\n",
    "\n",
    "\n",
    "    def stopword_remove(self, stopwords):\n",
    "\n",
    "        \"\"\"\n",
    "        Remove stopwords from tokens.\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        self.tokens = [t for t in self.tokens if t not in stopwords]\n",
    "\n",
    "\n",
    "    def stem(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Stem tokens with Porter Stemmer.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.stems = [PorterStemmer().stem(t) for t in self.tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs,re\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DocsCollection():\n",
    "    \n",
    "    def __init__(self, doc_data, stopword_file):\n",
    "\n",
    "        self.docs = [Doc(doc[2], doc[1], doc[0]) for doc in doc_data]\n",
    "\n",
    "        with codecs.open(stopword_file,'r','utf-8') as f: raw = f.read()\n",
    "        self.stopwords = set(raw.splitlines())\n",
    "\n",
    "        self.N = len(self.docs)\n",
    "        \n",
    "    def clean_docs(self, length):\n",
    "        for doc in self.docs:\n",
    "            doc.token_clean(length)\n",
    "            doc.stopword_remove(self.stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "fd = open('./data/pres_speech.json', 'r')\n",
    "text = fd.read()\n",
    "fd.close()\n",
    "\n",
    "pres_speech = json.loads(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "speech_docs = DocsCollection(pres_speech[0: 2], './data/stopwords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "speech_docs.clean_docs(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['please',\n",
       " 'everybody',\n",
       " 'seat',\n",
       " 'speaker',\n",
       " 'vice',\n",
       " 'president',\n",
       " 'members',\n",
       " 'congress',\n",
       " 'fellow',\n",
       " 'americans']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_docs.docs[0].tokens[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
