{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW2  Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class of Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "class Doc():\n",
    "    \n",
    "    \"\"\" The Doc class rpresents a class of individula documents\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, docid, doc, author, year):\n",
    "        self.docid = docid\n",
    "        self.text = doc.lower()\n",
    "        self.text = re.sub(u'[\\u2019\\']', '', self.text)\n",
    "        self.tokens = np.array(wordpunct_tokenize(self.text))\n",
    "        self.stem = None\n",
    "        self.author = author\n",
    "        self.year = year\n",
    "        \n",
    "    def tf(self, wordlist):\n",
    "        \n",
    "        \"\"\"\n",
    "        Returns ARRAY with wordlist frequency\n",
    "        \"\"\"\n",
    "        \n",
    "        count = np.zeros(len(wordlist))\n",
    "        \n",
    "        for wid, word in np.ndenumerate(wordlist):\n",
    "            count[wid] = (self.tokens == word).sum()\n",
    "        return count\n",
    "        \n",
    "    \n",
    "    def word_exists(self, wordlist):\n",
    "        \n",
    "        \"\"\"\n",
    "        Returns ARRAY of binary value where 1 inidicates presence of a word\n",
    "        \"\"\"\n",
    "        \n",
    "        is_word = np.zeros(len(wordlist))\n",
    "        \n",
    "        for wid, word in np.ndenumerate(wordlist):\n",
    "            if word in self.tokens:\n",
    "                is_word[wid] = 1\n",
    "        return is_word\n",
    "            \n",
    "    def token_clean(self,length):\n",
    "\n",
    "        \"\"\" \n",
    "        strip out non-alpha tokens and length one tokens\n",
    "        \"\"\"\n",
    "\n",
    "        self.tokens = np.array([t for t in self.tokens if (t.isalpha() and len(t) > length)])\n",
    "\n",
    "\n",
    "    def stopword_remove(self, stopwords):\n",
    "\n",
    "        \"\"\"\n",
    "        Remove stopwords from tokens.\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        self.tokens = np.array([t for t in self.tokens if t not in stopwords])\n",
    "\n",
    "\n",
    "    def stem(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Stem tokens with Porter Stemmer.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.stems = n.array([PorterStemmer().stem(t) for t in self.tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Class of Document Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "\n",
    "class RawDocs():\n",
    "    \n",
    "    docid = 0\n",
    "    \"\"\" The RawDocs class rpresents a class of document collections\n",
    "     \n",
    "    \"\"\"\n",
    "    def __init__(self, doc_data, stopword_file):\n",
    "\n",
    "        self.docs = []\n",
    "        for doc in doc_data :\n",
    "            self.docs.append(Doc(RawDocs.docid, doc[2], doc[1], doc[0]))\n",
    "            RawDocs.docid += 1\n",
    "\n",
    "        with codecs.open(stopword_file,'r','utf-8') as f: raw = f.read()\n",
    "        self.stopwords = set(raw.splitlines())\n",
    "\n",
    "        self.N = len(self.docs)\n",
    "        RawDocs.docid = 0\n",
    "        \n",
    "    def clean_docs(self, length):\n",
    "        \"\"\" \n",
    "        Applies stopword removal, token cleaning to docs\n",
    "        \"\"\"\n",
    "        for doc in self.docs:\n",
    "            doc.token_clean(length)\n",
    "            doc.stopword_remove(self.stopwords)\n",
    "            \n",
    "    def count(self, dictionary):\n",
    "        \"\"\" \n",
    "        word count frequency of dictionary in document collection\n",
    "        \"\"\"\n",
    "        \n",
    "        return ({(doc.docid, doc.year, doc.author) : \\\n",
    "                 doc.tf(dictionary) for doc in self.docs})\n",
    "    \n",
    "    def idf(self, dictionary):\n",
    "        \"\"\" \n",
    "        returns array of inverted document frequency for given dictionary \n",
    "        over collection of docs\n",
    "        \"\"\"\n",
    "        \n",
    "        is_word_docs = np.array([doc.word_exists(dictionary) for doc in self.docs])\n",
    "        \n",
    "        return(np.log(self.N / sum([is_word for is_word in is_word_docs])))\n",
    "    \n",
    "    def tf_idf(self, dictionary):\n",
    "        \"\"\" \n",
    "        returns tf-idf score of given dictionary of words for every document \n",
    "        \"\"\"\n",
    "        #tf and idf are calls to functions of the class Doc to calculate word frequency and inverse df respectively\n",
    "        tf = self.count(dictionary)\n",
    "        idf = self.idf(dictionary)\n",
    "        \n",
    "        tf_idf_docs = dict()\n",
    "        \n",
    "        for doc in self.docs:\n",
    "            tf_idf_docs[(doc.docid, doc.year, doc.author) ] = \\\n",
    "            np.log(tf[(doc.docid, doc.year, doc.author)] + 1) * idf\n",
    "            \n",
    "        return(tf_idf_docs)\n",
    "    \n",
    "    def rank_tfidf(self, dictionary):\n",
    "        \n",
    "        \"\"\"\n",
    "        Calculates document rank based on tfidf\n",
    "        \"\"\"\n",
    "        \n",
    "        docs_tfidf = self.tf_idf(dictionary)\n",
    "        \n",
    "        doc_rank = [[key, sum(docs_tfidf[key])] for key in docs_tfidf.keys()]\n",
    "        \n",
    "        doc_rank.sort(key=lambda x: x[1], reverse = True)\n",
    "        \n",
    "        return(doc_rank)   \n",
    "        #return(np.sort(np.array(doc_rank), axis=0)[::-1])\n",
    "    \n",
    "    def rank_count(self, dictionary):\n",
    "        \n",
    "        \"\"\"\n",
    "        Calculates document rank based on word frequency\n",
    "        \"\"\"\n",
    "        \n",
    "        docs_count = self.count(dictionary)\n",
    "        \n",
    "        doc_rank = [[key, sum(docs_count[key])] for key in docs_count.keys()]\n",
    "        \n",
    "        doc_rank.sort(key=lambda x: x[1], reverse = True)\n",
    "        \n",
    "        return(doc_rank)\n",
    "        #return(np.sort(np.array(doc_rank), axis=0)[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####import documents for consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import documents for consumption\n",
    "import json\n",
    "fd = open('./data/pres_speech.json', 'r')\n",
    "text = fd.read()\n",
    "fd.close()\n",
    "\n",
    "pres_speech = json.loads(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Instantiate RawDocs class and preprocess documents preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "speech_docs = RawDocs(pres_speech[0:20], './data/stopwords.txt')\n",
    "speech_docs.clean_docs(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, u'2013', u'Obama'): array([ 0.,  3.,  2.,  1.,  2.,  1.,  1.,  3.,  0.]),\n",
       " (1, u'2014', u'Obama'): array([ 1.,  5.,  1.,  3.,  0.,  3.,  1.,  7.,  1.]),\n",
       " (2, u'2009', u'Obama'): array([ 0.,  0.,  1.,  1.,  0.,  1.,  0.,  5.,  0.]),\n",
       " (3, u'2010', u'Obama'): array([ 1.,  0.,  2.,  2.,  1.,  2.,  2.,  4.,  0.]),\n",
       " (4, u'2011', u'Obama'): array([ 0.,  9.,  0.,  0.,  0.,  1.,  3.,  4.,  0.]),\n",
       " (5, u'2012', u'Obama'): array([ 1.,  1.,  2.,  0.,  2.,  0.,  0.,  6.,  1.]),\n",
       " (6,\n",
       "  u'2005',\n",
       "  u'Bush'): array([  1.,   0.,   5.,   2.,   0.,  11.,   0.,   5.,   2.]),\n",
       " (7, u'2006', u'Bush'): array([ 1.,  0.,  7.,  4.,  1.,  6.,  3.,  2.,  1.]),\n",
       " (8,\n",
       "  u'2007',\n",
       "  u'Bush'): array([  3.,   0.,   2.,   0.,   0.,   3.,   2.,  10.,   0.]),\n",
       " (9, u'2008', u'Bush'): array([ 1.,  0.,  2.,  0.,  1.,  8.,  0.,  3.,  2.]),\n",
       " (10, u'2001', u'Bush'): array([ 0.,  0.,  0.,  0.,  0.,  6.,  1.,  1.,  0.]),\n",
       " (11,\n",
       "  u'2002',\n",
       "  u'Bush'): array([  4.,   1.,   2.,   1.,   0.,   4.,   2.,  12.,   2.]),\n",
       " (12,\n",
       "  u'2003',\n",
       "  u'Bush'): array([  3.,   0.,   5.,   2.,   0.,   9.,   0.,  11.,   0.]),\n",
       " (13,\n",
       "  u'2004',\n",
       "  u'Bush'): array([  5.,   1.,   1.,   1.,   1.,   4.,   2.,  12.,   2.]),\n",
       " (14,\n",
       "  u'1997',\n",
       "  u'Clinton'): array([ 0.,  0.,  3.,  0.,  0.,  9.,  0.,  7.,  0.]),\n",
       " (15,\n",
       "  u'1998',\n",
       "  u'Clinton'): array([ 0.,  3.,  3.,  0.,  1.,  7.,  2.,  3.,  0.]),\n",
       " (16,\n",
       "  u'1999',\n",
       "  u'Clinton'): array([ 0.,  2.,  1.,  0.,  0.,  8.,  1.,  5.,  0.]),\n",
       " (17,\n",
       "  u'2000',\n",
       "  u'Clinton'): array([ 1.,  4.,  3.,  0.,  0.,  8.,  0.,  3.,  0.]),\n",
       " (18,\n",
       "  u'1993',\n",
       "  u'Clinton'): array([ 0.,  0.,  0.,  1.,  2.,  3.,  0.,  1.,  0.]),\n",
       " (19,\n",
       "  u'1994',\n",
       "  u'Clinton'): array([ 0.,  0.,  2.,  2.,  2.,  5.,  0.,  2.,  0.])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define sample\n",
    "sample_list = ['danger', 'race', 'human', 'fear', 'influence', 'peace', 'love', 'war', 'tyranny']\n",
    "#conut fucnction\n",
    "speech_docs.count(sample_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0,\n",
       "  u'2013',\n",
       "  u'Obama'): array([ 0.        ,  1.10696672,  0.17854529,  0.41438903,  0.87725037,\n",
       "         0.0355538 ,  0.41438903,  0.        ,  0.        ]),\n",
       " (1,\n",
       "  u'2014',\n",
       "  u'Obama'): array([ 0.41438903,  1.43073373,  0.11264954,  0.82877806,  0.        ,\n",
       "         0.0711076 ,  0.41438903,  0.        ,  0.72768125]),\n",
       " (2,\n",
       "  u'2009',\n",
       "  u'Obama'): array([ 0.        ,  0.        ,  0.11264954,  0.41438903,  0.        ,\n",
       "         0.0355538 ,  0.        ,  0.        ,  0.        ]),\n",
       " (3,\n",
       "  u'2010',\n",
       "  u'Obama'): array([ 0.41438903,  0.        ,  0.17854529,  0.65679108,  0.55348336,\n",
       "         0.05635144,  0.65679108,  0.        ,  0.        ]),\n",
       " (4,\n",
       "  u'2011',\n",
       "  u'Obama'): array([ 0.        ,  1.83863192,  0.        ,  0.        ,  0.        ,\n",
       "         0.0355538 ,  0.82877806,  0.        ,  0.        ]),\n",
       " (5,\n",
       "  u'2012',\n",
       "  u'Obama'): array([ 0.41438903,  0.55348336,  0.17854529,  0.        ,  0.87725037,\n",
       "         0.        ,  0.        ,  0.        ,  0.72768125]),\n",
       " (6,\n",
       "  u'2005',\n",
       "  u'Bush'): array([ 0.41438903,  0.        ,  0.29119483,  0.65679108,  0.        ,\n",
       "         0.12745905,  0.        ,  0.        ,  1.15334749]),\n",
       " (7,\n",
       "  u'2006',\n",
       "  u'Bush'): array([ 0.41438903,  0.        ,  0.33794861,  0.96218153,  0.55348336,\n",
       "         0.09981214,  0.82877806,  0.        ,  0.72768125]),\n",
       " (8,\n",
       "  u'2007',\n",
       "  u'Bush'): array([ 0.82877806,  0.        ,  0.17854529,  0.        ,  0.        ,\n",
       "         0.0711076 ,  0.65679108,  0.        ,  0.        ]),\n",
       " (9,\n",
       "  u'2008',\n",
       "  u'Bush'): array([ 0.41438903,  0.        ,  0.17854529,  0.        ,  0.55348336,\n",
       "         0.11270289,  0.        ,  0.        ,  1.15334749]),\n",
       " (10,\n",
       "  u'2001',\n",
       "  u'Bush'): array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.09981214,  0.41438903,  0.        ,  0.        ]),\n",
       " (11,\n",
       "  u'2002',\n",
       "  u'Bush'): array([ 0.96218153,  0.55348336,  0.17854529,  0.41438903,  0.        ,\n",
       "         0.08255337,  0.65679108,  0.        ,  1.15334749]),\n",
       " (12,\n",
       "  u'2003',\n",
       "  u'Bush'): array([ 0.82877806,  0.        ,  0.29119483,  0.65679108,  0.        ,\n",
       "         0.11810718,  0.        ,  0.        ,  0.        ]),\n",
       " (13,\n",
       "  u'2004',\n",
       "  u'Bush'): array([ 1.07118011,  0.55348336,  0.11264954,  0.41438903,  0.55348336,\n",
       "         0.08255337,  0.65679108,  0.        ,  1.15334749]),\n",
       " (14,\n",
       "  u'1997',\n",
       "  u'Clinton'): array([ 0.        ,  0.        ,  0.22529908,  0.        ,  0.        ,\n",
       "         0.11810718,  0.        ,  0.        ,  0.        ]),\n",
       " (15,\n",
       "  u'1998',\n",
       "  u'Clinton'): array([ 0.        ,  1.10696672,  0.22529908,  0.        ,  0.55348336,\n",
       "         0.10666141,  0.65679108,  0.        ,  0.        ]),\n",
       " (16,\n",
       "  u'1999',\n",
       "  u'Clinton'): array([ 0.        ,  0.87725037,  0.11264954,  0.        ,  0.        ,\n",
       "         0.11270289,  0.41438903,  0.        ,  0.        ]),\n",
       " (17,\n",
       "  u'2000',\n",
       "  u'Clinton'): array([ 0.41438903,  1.28514856,  0.22529908,  0.        ,  0.        ,\n",
       "         0.11270289,  0.        ,  0.        ,  0.        ]),\n",
       " (18,\n",
       "  u'1993',\n",
       "  u'Clinton'): array([ 0.        ,  0.        ,  0.        ,  0.41438903,  0.87725037,\n",
       "         0.0711076 ,  0.        ,  0.        ,  0.        ]),\n",
       " (19,\n",
       "  u'1994',\n",
       "  u'Clinton'): array([ 0.        ,  0.        ,  0.17854529,  0.65679108,  0.87725037,\n",
       "         0.09190525,  0.        ,  0.        ,  0.        ])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tf idf function\n",
    "speech_docs.tf_idf(sample_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(13, u'2004', u'Bush'), 4.5978773281949756],\n",
       " [(11, u'2002', u'Bush'), 4.0012911525378563],\n",
       " [(1, u'2014', u'Obama'), 3.999728240209798],\n",
       " [(7, u'2006', u'Bush'), 3.9242739884052078],\n",
       " [(0, u'2013', u'Obama'), 3.0270942427285847],\n",
       " [(5, u'2012', u'Obama'), 2.7513492962309956],\n",
       " [(4, u'2011', u'Obama'), 2.7029637833547655],\n",
       " [(15, u'1998', u'Clinton'), 2.649201633214],\n",
       " [(6, u'2005', u'Bush'), 2.643181473218295],\n",
       " [(3, u'2010', u'Obama'), 2.5163512777260344],\n",
       " [(9, u'2008', u'Bush'), 2.4124680568551571],\n",
       " [(17, u'2000', u'Clinton'), 2.0375395537903396],\n",
       " [(12, u'2003', u'Bush'), 1.8948711445507287],\n",
       " [(19, u'1994', u'Clinton'), 1.8044919823257539],\n",
       " [(8, u'2007', u'Bush'), 1.7352220365271203],\n",
       " [(16, u'1999', u'Clinton'), 1.5169918240189491],\n",
       " [(18, u'1993', u'Clinton'), 1.3627470039418017],\n",
       " [(2, u'2009', u'Obama'), 0.56259237166353726],\n",
       " [(10, u'2001', u'Bush'), 0.51420117363538864],\n",
       " [(14, u'1997', u'Clinton'), 0.34340625056533536]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get ranked docs usinf tf idf\n",
    "speech_docs.rank_tfidf(sample_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(12, u'2003', u'Bush'), 30.0],\n",
       " [(13, u'2004', u'Bush'), 29.0],\n",
       " [(11, u'2002', u'Bush'), 28.0],\n",
       " [(6, u'2005', u'Bush'), 26.0],\n",
       " [(7, u'2006', u'Bush'), 25.0],\n",
       " [(1, u'2014', u'Obama'), 22.0],\n",
       " [(8, u'2007', u'Bush'), 20.0],\n",
       " [(17, u'2000', u'Clinton'), 19.0],\n",
       " [(15, u'1998', u'Clinton'), 19.0],\n",
       " [(14, u'1997', u'Clinton'), 19.0],\n",
       " [(4, u'2011', u'Obama'), 17.0],\n",
       " [(9, u'2008', u'Bush'), 17.0],\n",
       " [(16, u'1999', u'Clinton'), 17.0],\n",
       " [(3, u'2010', u'Obama'), 14.0],\n",
       " [(0, u'2013', u'Obama'), 13.0],\n",
       " [(19, u'1994', u'Clinton'), 13.0],\n",
       " [(5, u'2012', u'Obama'), 13.0],\n",
       " [(10, u'2001', u'Bush'), 8.0],\n",
       " [(2, u'2009', u'Obama'), 8.0],\n",
       " [(18, u'1993', u'Clinton'), 7.0]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get ranked docs usinf count\n",
    "speech_docs.rank_count(sample_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there is an expected difference in ranking between tf idf and tf methods. For a more detailed explanation of why we expect this result please refer to python ntebook for question 2 week 2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
