{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#open url information\n",
      "fd = open('./data/project_successful_meta.json', 'r')\n",
      "text = fd.read()\n",
      "fd.close()\n",
      "\n",
      "pmeta = json.loads(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from math import log\n",
      "\n",
      "class Doc():\n",
      "    \n",
      "    def __init__(self, doc):\n",
      "        self.text = doc[2].lower()\n",
      "        self.text = re.sub(u'[\\u2019\\']', '', self.text)\n",
      "        self.tokens = None\n",
      "        self.stem = None\n",
      "        __tokenize()\n",
      "        \n",
      "    def __tokenize(self):\n",
      "        \n",
      "        \"\"\"\n",
      "        Tokenize the doc\n",
      "        \"\"\"\n",
      "        \n",
      "        self.tokens = wordpunct_tokenize(self.text)\n",
      "    \n",
      "    def count(self, wordlist):\n",
      "        \n",
      "        \"\"\"\n",
      "        Returns dict woth wordlist frequency\n",
      "        \"\"\"\n",
      "        \n",
      "        count = dict()\n",
      "        for word in wordlist:\n",
      "            if word in log_count.keys():\n",
      "                count[token] += 1\n",
      "            else:\n",
      "                count[token] =  1\n",
      "        \n",
      "    \n",
      "    def log_count(self, wordlist):\n",
      "        \n",
      "        \"\"\"\n",
      "        Return dict with log count of wordlist\n",
      "        \"\"\"\n",
      "        \n",
      "        self.log_count = get_count()\n",
      "        for token in cnt.keys():\n",
      "            self.log_count[token] = log(1 + self.log_count[token])\n",
      "            \n",
      "    def token_clean(self,length):\n",
      "\n",
      "\t\t\"\"\" \n",
      "\t\tstrip out non-alpha tokens and length one tokens\n",
      "\t\t\"\"\"\n",
      "\n",
      "\t\tself.tokens = [t for t in self.tokens if t.isalpha() == 1 and len(t) > length]\n",
      "\n",
      "\n",
      "    def stopword_remove(self, stopwords):\n",
      "\n",
      "\t\t\"\"\"\n",
      "\t\tRemove stopwords from tokens.\n",
      "\t\t\"\"\"\n",
      "\n",
      "\t\t\n",
      "\t\tself.tokens = [t for t in self.tokens if t not in stopwords]\n",
      "\n",
      "\n",
      "    def stem(self):\n",
      "\n",
      "\t\t\"\"\"\n",
      "\t\tStem tokens with Porter Stemmer.\n",
      "\t\t\"\"\"\n",
      "\n",
      "\t\t\n",
      "\t\tself.stems = [PorterStemmer().stem(t) for t in self.tokens]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import codecs,re\n",
      "from nltk.tokenize import wordpunct_tokenize\n",
      "from nltk import PorterStemmer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class DocsCollection():\n",
      "    \n",
      "    def __init__(self, doc_data, stopword_file):\n",
      "\n",
      "\t\tself.docs = [Doc(doc) for doc in doc_data]\n",
      "\n",
      "\t\twith codecs.open(stopword_file,'r','utf-8') as f: raw = f.read()\n",
      "\t\tself.stopwords = set(raw.splitlines())\n",
      "\n",
      "\t\tself.N = len(self.docs)\n",
      "        \n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "fd = open('./data/pres_speech.json', 'r')\n",
      "text = fd.read()\n",
      "fd.close()\n",
      "\n",
      "pres_speech = json.loads(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "speech_docs = DocsCollection(pres_speech[0: 2], './data/stopwords.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "global name '_tokenize' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-71-13f12f7350d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspeech_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDocsCollection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpres_speech\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./data/stopwords.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-64-2968a4724a0b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, doc_data, stopword_file)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopword_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopword_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-70-b4e94a03f083>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'[\\u2019\\']'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: global name '_tokenize' is not defined"
       ]
      }
     ],
     "prompt_number": 71
    }
   ],
   "metadata": {}
  }
 ]
}